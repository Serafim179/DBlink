{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c0c707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from DataHandlers import *\n",
    "from NN_model import *\n",
    "from Trainers import *\n",
    "from Utils import *\n",
    "from demo_exp_params import *\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "tmp_result_dir_exist = os.path.exists(\"./tmp_results\")\n",
    "if not tmp_result_dir_exist:\n",
    "   # Create a tmp_results dir because it does not exist\n",
    "   os.makedirs(\"./tmp_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1079bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step I - Parameter Initialization #######\n",
    "# Run flags\n",
    "GenerateTrainData = True\n",
    "GenerateTestData = True\n",
    "TrainNetFlag = False\n",
    "TestOnRealData = False\n",
    "\n",
    "path = r'./' # Path to model\n",
    "model_name = 'LSTM_model' # Model name\n",
    "scale = 4 # Scale factor, the size of the reconstructed image pixels\n",
    "sum_factor = 10 # The temporal window size DBlink uses to sum localizations\n",
    "pixel_size = 160 # Camera pixel size - relevant for experimental data\n",
    "simulated_video_length = 3000 # Length of simulated video - relevant for simulated data generation\n",
    "density = 0.002 # Blinking density (percentage out of the number of non-zero pixels in the simulated structure)\n",
    "img_size = 32 # Simulated image size - relevant for simulated data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e659b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step II - Training data generation #######\n",
    "trainset_size = 1024\n",
    "valset_size = 256\n",
    "if(TrainNetFlag):\n",
    "    if(GenerateTrainData):\n",
    "        [X_train, y_train] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=trainset_size,\n",
    "                                                        video_length=simulated_video_length, emitters_density=density,\n",
    "                                                         scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        [X_val, y_val] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=valset_size,\n",
    "                                                    video_length=simulated_video_length, emitters_density=density,\n",
    "                                                    scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "        torch.save(X_train, 'X_train')\n",
    "        torch.save(y_train, 'y_train')\n",
    "        torch.save(X_val, 'X_val')\n",
    "        torch.save(y_val, 'y_val')\n",
    "    else:\n",
    "        X_train = torch.load('X_train')\n",
    "        y_train = torch.load('y_train')\n",
    "        X_val = torch.load('X_val')\n",
    "        y_val = torch.load('y_val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538e9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step III - Build Model, loss and optimizer #######\n",
    "num_layers = 2 # The number of LSTM layers\n",
    "hidden_channels = 4 # The hidden layer number of channels\n",
    "lr = 1e-4 # Training learning rate\n",
    "window_size = 25 # The number of used windows (in each direction) for the inference of each reconstructed frame\n",
    "betas = (0.99, 0.999) # Parameters of Adam optimizer\n",
    "batch_size = 1\n",
    "epochs = 150\n",
    "patience = 3\n",
    "\n",
    "model = ConvOverlapBLSTM(input_size=(img_size, img_size), input_channels=1, hidden_channels=hidden_channels, num_layers=num_layers, device=device).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=patience, min_lr=1e-9, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8062de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Step IV - Training the model #######\n",
    "if(TrainNetFlag):\n",
    "    dl_train = CreateDataLoader(X_train, y_train, batch_size=batch_size)\n",
    "    dl_val = CreateDataLoader(X_val, y_val, batch_size=batch_size)\n",
    "\n",
    "    trainer = LSTM_overlap_Trainer(model, criterion, optimizer, scheduler, batch_size, window_size=window_size,\n",
    "                                   vid_length=X_train.shape[1], patience=patience, device=device)\n",
    "    trainer.fit(dl_train, dl_val, num_epochs=epochs)\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_name, map_location=torch.device(device)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716d34ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Generating training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:40<00:00, 10.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:37<00:00,  7.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 12533.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 1\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:27<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 1\n",
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:32<00:00,  9.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 10717.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 2\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [01:07<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 2\n",
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:32<00:00,  9.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 14321.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 3\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 3\n",
      "Forward pass through the network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:30<00:00,  9.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 14317.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 4\n",
      "Post processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [01:20<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Completed vid 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Step V - Testing the model #######\n",
    "if(TestOnRealData):\n",
    "    analyze_storm_exp_overlap(path_to_model='./{}'.format(model_name),\n",
    "                              exp_class=demo_params(),\n",
    "                              hidden_channels=hidden_channels,\n",
    "                              num_layers=num_layers,\n",
    "                              scale=scale,\n",
    "                              device=device)\n",
    "    post_process_results(r'./tmp_results', 1)\n",
    "else:\n",
    "    testset_size = 4\n",
    "    if(GenerateTestData):\n",
    "        [X_test, y_test] = Simulate_Train_Data_060622(obs_size=img_size, dataset_size=testset_size,\n",
    "                                                      video_length=simulated_video_length, emitters_density=density,\n",
    "                                                      scale=scale, sum_range=sum_factor, datatype='tubules')\n",
    "        X_test = torch.FloatTensor(X_test)\n",
    "        y_test = torch.FloatTensor(y_test)\n",
    "        torch.save(X_test, 'X_test')\n",
    "        torch.save(y_test, 'y_test')\n",
    "    else:\n",
    "        X_test = torch.load('X_test')\n",
    "        y_test = torch.load('y_test')\n",
    "\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "    N, T, C, H, W = X_test.shape\n",
    "\n",
    "    model = ConvOverlapBLSTM(input_size=(img_size, img_size), input_channels=1, hidden_channels=hidden_channels,\n",
    "                                num_layers=num_layers, device=device).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(path, model_name), map_location=torch.device(device)))\n",
    "\n",
    "    down = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    up = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    out_ind = torch.zeros(X_test.size(1), requires_grad=False, dtype=torch.int)\n",
    "    for i in range(X_test.size(1)):\n",
    "        down[i] = torch.max(torch.IntTensor([0, i - sum_factor * window_size]))\n",
    "        up[i] = torch.min(torch.IntTensor([X_test.size(1), i + sum_factor * window_size]))\n",
    "        out_ind[i] = i - down[i]\n",
    "\n",
    "    for i in range(X_test.size(0)):\n",
    "        out = []\n",
    "        print('Forward pass through the network')\n",
    "        with torch.no_grad():\n",
    "            for j in tqdm(range(X_test.shape[1])):\n",
    "                curr_out = model(X_test[i:i + 1, down[j]:up[j]:sum_factor],\n",
    "                                 torch.flip(X_test[i:i + 1, down[j]:up[j]:sum_factor], dims=[1]))\n",
    "                curr_out = curr_out.detach().cpu()[0, int(out_ind[j] / sum_factor)]\n",
    "                out.append(curr_out)\n",
    "\n",
    "        out = torch.stack(out, dim=1)\n",
    "\n",
    "        curr_vid = np.zeros([1, X_test.size(1), C, H, W])\n",
    "        for j in tqdm(range(X_test.size(1))):\n",
    "            curr_vid[0, j] = 255 * normalize_input_01(out[0, j].numpy())\n",
    "\n",
    "        np.save('tmp_results/np_vid_{}'.format(i + 1), curr_vid[0, :-2*window_size])\n",
    "        np.save('tmp_results/gt_vid_{}'.format(i + 1), y_test[i, :-2*window_size].detach().cpu().numpy())\n",
    "\n",
    "        print(\"-I- Completed vid\", i + 1)\n",
    "\n",
    "        # Post process reconstruction and generate output video\n",
    "        post_process_results(r'./tmp_results', i + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34375064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "          2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "         38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], dtype=torch.int32),\n",
       " tensor([250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
       "         264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277,\n",
       "         278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291,\n",
       "         292, 293, 294, 295, 296, 297, 298, 299, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,\n",
       "         300, 300, 300, 300, 300, 300], dtype=torch.int32),\n",
       " 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down, up, sum_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8edbaac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 1, 128, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e71b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
